scrapy shell
fetch("https://www.reddit.com/r/gameofthrones/")
print(response.text)

titles = response.css('.djQeMY').extract()
votes = response.css('.bLdsNs').extract()
times = response.css('._3jOxDPIQ0KaOWpzvSQo-1s').extract()
comments = response.css('.FHCV02u6Cp2zYL0fhQPsO').extract()


#Writing Custom Spiders
scrapy startproject ourfirstscraper
cd ourfirstscraper
scrapy genspider redditbot www.reddit.com/r/gameofthrones/

# -*- coding: utf-8 -*-
import scrapy


class RedditbotSpider(scrapy.Spider):
    name = 'redditbot'
    allowed_domains = ['www.reddit.com/r/gameofthrones/']
    start_urls = ['http://www.reddit.com/r/gameofthrones/']

    def parse(self, response):
        #Extracting the content using css selectors
        titles = response.css('.djQeMY').extract()
        votes = response.css('.hhIiyy').extract()
        times = response.css('._3jOxDPIQ0KaOWpzvSQo-1s').extract()
        comments = response.css('.FHCV02u6Cp2zYL0fhQPsO').extract()
       
        #Give the extracted content row wise
        for item in zip(titles,votes,times,comments):
            #create a dictionary to store the scraped info
            scraped_info = {
                'title' : item[0],
                'vote' : item[1],
                'created_at' : item[2],
                'comments' : item[3],
            }

            #yield or give the scraped info to scrapy
            yield scraped_info

Open the settings.py file and add the following code to it:
#Export as CSV Feed
FEED_FORMAT = "csv"
FEED_URI = "reddit.csv"

scrapy crawl redditbot


#Scraping an E-Commerce site
scrapy genspider shopclues https://www.shopclues.com/cricket-bats.html

#Image handling
#Add the following lines to the settings.py file :

ITEM_PIPELINES = {
  'scrapy.pipelines.images.ImagesPipeline': 1
}
IMAGES_STORE = 'tmp/images/'


import scrapy

class ShopcluesSpider(scrapy.Spider):
   #name of spider
   name = 'shopclues'

   #list of allowed domains
   allowed_domains = ['www.shopclues.com/cricket-bats.html']
   #starting url
   start_urls = ['https://www.shopclues.com/cricket-bats.html']
   #location of csv file
   custom_settings = {
       'FEED_URI' : 'tmp/shopclues.csv'
   }


   def parse(self, response):
       #Extract product information
       titles = response.css('img::attr(title)').extract()
       images = response.css('img::attr(data-img)').extract()
       prices = response.css('.p_price::text').extract()
       discounts = response.css('.prd_discount::text').extract()


       for item in zip(titles,prices,images,discounts):
           scraped_info = {
               'title' : item[0],
               'price' : item[1],
               'image_urls' : [item[2]], #Set's the url for scrapy to download images
               'discount' : item[3]
           }

           yield scraped_info
		   

		   
		   
Scrapy genspider techcrunch techcrunch.com/feed/
import scrapy

class TechcrunchSpider(scrapy.Spider):
    #name of the spider
    name = 'techcrunch'

    #list of allowed domains
    allowed_domains = ['techcrunch.com/feed/']

    #starting url for scraping
    start_urls = ['http://techcrunch.com/feed/']

    #setting the location of the output csv file
    custom_settings = {
        'FEED_URI' : 'tmp/techcrunch.csv'
    }

    def parse(self, response):
        #Remove XML namespaces
        response.selector.remove_namespaces()

        #Extract article information
        titles = response.xpath('//item/title/text()').extract()
        authors = response.xpath('//item/creator/text()').extract()
        dates = response.xpath('//item/pubDate/text()').extract()
        links = response.xpath('//item/link/text()').extract()

        for item in zip(titles,authors,dates,links):
            scraped_info = {
                'title' : item[0],
                'author' : item[1],
                'publish_date' : item[2],
                'link' : item[3]
            }

            yield scraped_info

			
scrapy crawl techcrunch